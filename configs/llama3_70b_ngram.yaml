# Llama3-70B-Instruct with N-gram Self-Drafting Speculative Decoding
model: meta-llama/Meta-Llama-3-70B-Instruct

speculative_decoding:
  enabled: true
  model: "[ngram]"  # Self-drafting using n-gram matching from prompt
  num_tokens: 5
  ngram_prompt_lookup_max: 4  # Maximum n-gram length to match

generation:
  max_tokens: 512
  temperature: 0.7
  top_p: 1.0
  top_k: -1

engine:
  tensor_parallel_size: 4  # Adjust based on available GPUs (70B typically needs 4+ GPUs)
  gpu_memory_utilization: 0.9
  trust_remote_code: false
