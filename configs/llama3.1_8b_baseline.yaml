# Llama3.1-8B-Instruct Baseline (no speculative decoding)
model: meta-llama/Meta-Llama-3.1-8B-Instruct

generation:
  max_tokens: 512
  temperature: 0.7
  top_p: 1.0
  top_k: -1

engine:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.85  # Reduced to leave memory for sampler warmup
  max_model_len: 8192  # Reduced to fit memory constraints
  trust_remote_code: false
  # extra_params:
  #   max_num_seqs: 64  # Reduce from default 256 to lower memory usage during warmup
