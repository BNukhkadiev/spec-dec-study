# Example configuration with speculative decoding
model: meta-llama/Llama-2-7b-chat-hf

speculative_decoding:
  enabled: true
  model: meta-llama/Llama-2-7b-chat-hf  # Draft model
  num_tokens: 5  # Number of speculative tokens

generation:
  max_tokens: 512
  temperature: 0.7
  top_p: 1.0
  top_k: -1

engine:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
  trust_remote_code: false
